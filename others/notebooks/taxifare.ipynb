{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd31a270-4ad0-4ba6-9f3b-cb36f89f50fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d59b0258-9632-4cac-95e6-728a60d17e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0a85d1f-616a-4e0b-8907-ecf6c9add4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow import feature_column as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7c19cea-cdd8-4643-80c1-244ed578c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f037cb8-cf91-4f1b-be5d-3f174af59343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fba0ad1d-3457-4ae3-8118-c2482e9a7dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = [\n",
    "        'fare_amount',\n",
    "        'pickup_datetime',\n",
    "        'pickup_longitude',\n",
    "        'pickup_latitude',\n",
    "        'dropoff_longitude',\n",
    "        'dropoff_latitude',\n",
    "        'passenger_count',\n",
    "        'key',\n",
    "]\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "DEFAULTS = [[0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]\n",
    "DAYS = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c60361dd-0a21-4f3f-9c4c-2a34ff66dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=None)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "220f4b97-d83c-4304-aea7-308e105ae5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return tf.data.Dataset\n",
    "def load_dataset(pattern, batch_size, num_repeat):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS,\n",
    "        num_epochs=num_repeat,\n",
    "    )\n",
    "    return dataset.map(features_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f672fe66-454a-4cec-b305-298fba31e6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_and_labels(row_data):\n",
    "    for unwanted_col in ['key']:\n",
    "        row_data.pop(unwanted_col)\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd8b493-9a30-46c3-81df-441d049dcd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed2f169-a9cd-479c-84f3-daa5926d9c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE] --eval_data_path\n",
      "                             EVAL_DATA_PATH [--nnsize NNSIZE [NNSIZE ...]]\n",
      "                             [--nbuckets NBUCKETS] [--lr LR]\n",
      "                             [--num_evals NUM_EVALS]\n",
      "                             [--num_examples_to_train_on NUM_EXAMPLES_TO_TRAIN_ON]\n",
      "                             --output_dir OUTPUT_DIR --train_data_path\n",
      "                             TRAIN_DATA_PATH [--job-dir JOB_DIR]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --eval_data_path, --output_dir, --train_data_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanchunyang/Documents/virenv/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--batch_size\",\n",
    "    help=\"Batch size for training steps\",\n",
    "    type=int,\n",
    "    default=32\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_data_path\",\n",
    "    help=\"GCS location pattern of eval files\",\n",
    "    required=True\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--nnsize\",\n",
    "    help=\"Hidden layer sizes (provide space-separated sizes)\",\n",
    "    nargs=\"+\",\n",
    "    type=int,\n",
    "    default=[32, 8]\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--nbuckets\",\n",
    "    help=\"Number of buckets to divide lat and lon with\",\n",
    "    type=int,\n",
    "    default=10\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr\",\n",
    "    help = \"learning rate for optimizer\",\n",
    "    type = float,\n",
    "    default = 0.001\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_evals\",\n",
    "    help=\"Number of times to evaluate model on eval data training.\",\n",
    "    type=int,\n",
    "    default=5\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_examples_to_train_on\",\n",
    "    help=\"Number of examples to train on.\",\n",
    "    type=int,\n",
    "    default=100\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_dir\",\n",
    "    help=\"GCS location to write checkpoints and export models\",\n",
    "    required=True\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_data_path\",\n",
    "    help=\"GCS location pattern of train files containing eval URLs\",\n",
    "    required=True\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--job-dir\",\n",
    "    help=\"this model ignores this field, but it is required by gcloud\",\n",
    "    default=\"junk\"\n",
    ")\n",
    "args = parser.parse_args()\n",
    "hparams = args.__dict__\n",
    "hparams.pop(\"job-dir\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbb5a0fb-f884-45dd-a746-7f6a263f8c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds = create_train_dataset(\"./taxi-train.csv\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d65982f-93d0-490a-96f3-b4723a4dfb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "traintest = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=\"./taxi-train.csv\",\n",
    "        batch_size=32,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS,\n",
    "        num_epochs=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a25f80e-635d-49e0-808e-c4dbd71a8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = traintest.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1ca4d1d-cc8d-4dd2-9186-c9f486d78b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TakeDataset element_spec=OrderedDict([('fare_amount', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('pickup_datetime', TensorSpec(shape=(32,), dtype=tf.string, name=None)), ('pickup_longitude', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('pickup_latitude', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('dropoff_longitude', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('dropoff_latitude', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('passenger_count', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('key', TensorSpec(shape=(32,), dtype=tf.string, name=None))])>\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0d13b6d-7e70-4f5f-a197-955ddaf127f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = example.map(features_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "049ced72-4f3f-4244-8913-c134ae98e765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_MapDataset element_spec=(OrderedDict([('pickup_datetime', TensorSpec(shape=(32,), dtype=tf.string, name=None)), ('pickup_longitude', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('pickup_latitude', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('dropoff_longitude', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('dropoff_latitude', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('passenger_count', TensorSpec(shape=(32,), dtype=tf.float32, name=None))]), TensorSpec(shape=(32,), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f7706c7-c29e-48be-bba7-0cb65bb3079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxi-train.csv taxi-valid.csv taxifare.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa14678f-d598-4077-81c2-c1dac55432b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1000 taxi-train.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l taxi-train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7f88a9a-6f82-4e20-b53d-877f1ecfb7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f30fb95a-36ed-4501-893d-fd0667434a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"taxi-train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96fc66e0-cc18-4972-8dfc-b83eb7781d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>11.3</th>\n",
       "      <th>2011-01-28 20:42:59 UTC</th>\n",
       "      <th>-73.999022</th>\n",
       "      <th>40.739146</th>\n",
       "      <th>-73.990369</th>\n",
       "      <th>40.717866</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2011-06-27 04:28:06 UTC</td>\n",
       "      <td>-73.987443</td>\n",
       "      <td>40.729221</td>\n",
       "      <td>-73.979013</td>\n",
       "      <td>40.758641</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.5</td>\n",
       "      <td>2011-04-03 00:54:53 UTC</td>\n",
       "      <td>-73.982539</td>\n",
       "      <td>40.735725</td>\n",
       "      <td>-73.954797</td>\n",
       "      <td>40.778388</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.2</td>\n",
       "      <td>2009-04-10 04:11:56 UTC</td>\n",
       "      <td>-74.001945</td>\n",
       "      <td>40.740505</td>\n",
       "      <td>-73.913850</td>\n",
       "      <td>40.758559</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.5</td>\n",
       "      <td>2014-02-24 18:22:00 UTC</td>\n",
       "      <td>-73.993372</td>\n",
       "      <td>40.753382</td>\n",
       "      <td>-73.860900</td>\n",
       "      <td>40.732897</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.9</td>\n",
       "      <td>2011-12-10 00:25:23 UTC</td>\n",
       "      <td>-73.996237</td>\n",
       "      <td>40.721848</td>\n",
       "      <td>-73.989416</td>\n",
       "      <td>40.718052</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2012-09-01 14:30:19 UTC</td>\n",
       "      <td>-73.977048</td>\n",
       "      <td>40.758461</td>\n",
       "      <td>-73.984899</td>\n",
       "      <td>40.744693</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.5</td>\n",
       "      <td>2012-11-08 13:28:07 UTC</td>\n",
       "      <td>-73.969402</td>\n",
       "      <td>40.757545</td>\n",
       "      <td>-73.950049</td>\n",
       "      <td>40.776079</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2014-07-15 11:37:25 UTC</td>\n",
       "      <td>-73.979318</td>\n",
       "      <td>40.760949</td>\n",
       "      <td>-73.957670</td>\n",
       "      <td>40.773724</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.3</td>\n",
       "      <td>2009-11-09 18:06:58 UTC</td>\n",
       "      <td>-73.955675</td>\n",
       "      <td>40.779154</td>\n",
       "      <td>-73.961172</td>\n",
       "      <td>40.772368</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2014-09-14 21:52:28 UTC</td>\n",
       "      <td>-73.993789</td>\n",
       "      <td>40.749181</td>\n",
       "      <td>-73.951233</td>\n",
       "      <td>40.770045</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   11.3  2011-01-28 20:42:59 UTC  -73.999022  40.739146  -73.990369  \\\n",
       "0   7.7  2011-06-27 04:28:06 UTC  -73.987443  40.729221  -73.979013   \n",
       "1  10.5  2011-04-03 00:54:53 UTC  -73.982539  40.735725  -73.954797   \n",
       "2  16.2  2009-04-10 04:11:56 UTC  -74.001945  40.740505  -73.913850   \n",
       "3  33.5  2014-02-24 18:22:00 UTC  -73.993372  40.753382  -73.860900   \n",
       "4   6.9  2011-12-10 00:25:23 UTC  -73.996237  40.721848  -73.989416   \n",
       "5   6.1  2012-09-01 14:30:19 UTC  -73.977048  40.758461  -73.984899   \n",
       "6   9.5  2012-11-08 13:28:07 UTC  -73.969402  40.757545  -73.950049   \n",
       "7   9.0  2014-07-15 11:37:25 UTC  -73.979318  40.760949  -73.957670   \n",
       "8   3.3  2009-11-09 18:06:58 UTC  -73.955675  40.779154  -73.961172   \n",
       "9  17.0  2014-09-14 21:52:28 UTC  -73.993789  40.749181  -73.951233   \n",
       "\n",
       "   40.717866  1   0  \n",
       "0  40.758641  1   1  \n",
       "1  40.778388  1   2  \n",
       "2  40.758559  1   3  \n",
       "3  40.732897  2   4  \n",
       "4  40.718052  1   5  \n",
       "5  40.744693  2   6  \n",
       "6  40.776079  1   7  \n",
       "7  40.773724  1   8  \n",
       "8  40.772368  1   9  \n",
       "9  40.770045  2  10  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3b85df7-11a6-4f6e-8a8e-ce58baf0b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(inputs, NUMERIC_COLS, STRING_COLS, nbuckets):\n",
    "    # Pass-through columns\n",
    "    transformed = inputs.copy()\n",
    "    del transformed['pickup_datetime']\n",
    "\n",
    "    feature_columns = {\n",
    "        colname: fc.numeric_column(colname)\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "\n",
    "    # Scaling longitude from range [-70, -78] to [0, 1]\n",
    "    for lon_col in ['pickup_longitude', 'dropoff_longitude']:\n",
    "        transformed[lon_col] = layers.Lambda(\n",
    "            lambda x: (x + 78)/8.0,\n",
    "            name='scale_{}'.format(lon_col)\n",
    "        )(inputs[lon_col])\n",
    "\n",
    "    # Scaling latitude from range [37, 45] to [0, 1]\n",
    "    for lat_col in ['pickup_latitude', 'dropoff_latitude']:\n",
    "        transformed[lat_col] = layers.Lambda(\n",
    "            lambda x: (x - 37)/8.0,\n",
    "            name='scale_{}'.format(lat_col)\n",
    "        )(inputs[lat_col])\n",
    "\n",
    "    # Adding Euclidean dist (no need to be accurate: NN will calibrate it)\n",
    "    transformed['euclidean'] = layers.Lambda(euclidean, name='euclidean')([\n",
    "        inputs['pickup_longitude'],\n",
    "        inputs['pickup_latitude'],\n",
    "        inputs['dropoff_longitude'],\n",
    "        inputs['dropoff_latitude']\n",
    "    ])\n",
    "    feature_columns['euclidean'] = fc.numeric_column('euclidean')\n",
    "\n",
    "    # hour of day from timestamp of form '2010-02-08 09:17:00+00:00'\n",
    "    transformed['hourofday'] = layers.Lambda(\n",
    "        lambda x: tf.strings.to_number(\n",
    "            tf.strings.substr(x, 11, 2), out_type=tf.dtypes.int32),\n",
    "        name='hourofday'\n",
    "    )(inputs['pickup_datetime'])\n",
    "    feature_columns['hourofday'] = fc.indicator_column(\n",
    "        fc.categorical_column_with_identity(\n",
    "            'hourofday', num_buckets=24))\n",
    "\n",
    "    latbuckets = np.linspace(0, 1, nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(0, 1, nbuckets).tolist()\n",
    "    b_plat = fc.bucketized_column(\n",
    "        feature_columns['pickup_latitude'], latbuckets)\n",
    "    b_dlat = fc.bucketized_column(\n",
    "            feature_columns['dropoff_latitude'], latbuckets)\n",
    "    b_plon = fc.bucketized_column(\n",
    "            feature_columns['pickup_longitude'], lonbuckets)\n",
    "    b_dlon = fc.bucketized_column(\n",
    "            feature_columns['dropoff_longitude'], lonbuckets)\n",
    "    ploc = fc.crossed_column(\n",
    "            [b_plat, b_plon], nbuckets * nbuckets)\n",
    "    dloc = fc.crossed_column(\n",
    "            [b_dlat, b_dlon], nbuckets * nbuckets)\n",
    "    pd_pair = fc.crossed_column([ploc, dloc], nbuckets ** 4)\n",
    "    feature_columns['pickup_and_dropoff'] = fc.embedding_column(\n",
    "            pd_pair, 100)\n",
    "\n",
    "    return transformed, feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29f2aed9-b5fe-474d-a255-c2023a40807b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TakeDataset element_spec=OrderedDict([('fare_amount', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('pickup_datetime', TensorSpec(shape=(32,), dtype=tf.string, name=None)), ('pickup_longitude', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('pickup_latitude', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('dropoff_longitude', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('dropoff_latitude', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('passenger_count', TensorSpec(shape=(32,), dtype=tf.float32, name=None)), ('key', TensorSpec(shape=(32,), dtype=tf.string, name=None))])>\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b239ee2-0f43-44c1-ba60-2351dccde245",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_COLS = ['pickup_datetime']\n",
    "NUMERIC_COLS = (\n",
    "            set(CSV_COLUMNS) - set([LABEL_COLUMN, 'key']) - set(STRING_COLS)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe50ed46-97d1-4d4c-8122-ecd689721b77",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.compact'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompact\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf1\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.compact'"
     ]
    }
   ],
   "source": [
    "import tensorflow.compact.v1 as tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e28174b6-7060-4ca6-8cfc-8002457dfc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_feature_columns(feature_columns, inputs):\n",
    "  # This is a convenient way to call a `feature_column` outside of an estimator\n",
    "  # to display its output.\n",
    "  feature_layer = tf1.keras.layers.DenseFeatures(feature_columns)\n",
    "  return feature_layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56585d-a5c2-43ae-b139-9dd0e93feeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assume 'feature1' and 'feature2' are original features\n",
    "feature1 = tf.feature_column.numeric_column(\"feature1\")\n",
    "feature2 = tf.feature_column.numeric_column(\"feature2\")\n",
    "\n",
    "# Custom lambda function to combine features\n",
    "combine_features = tf.keras.layers.Lambda(lambda x: tf.math.multiply(x[:, 0], x[:, 1]))\n",
    "\n",
    "# Use Lambda layer to create a new feature\n",
    "combined_feature = combine_features([feature1, feature2])\n",
    "\n",
    "# Apply further transformations using tf.feature_column\n",
    "# For example, create a bucketized column\n",
    "combined_feature_bucketized = tf.feature_column.bucketized_column(combined_feature, boundaries=[0, 10, 20, 30])\n",
    "\n",
    "# Continue with the rest of the feature columns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
